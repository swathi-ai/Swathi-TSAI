{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA7_Assignment_0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt2hRkZ9c854"
      },
      "source": [
        "### **1. What are Channels and Kernels (according to EVA)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB_7kF2xc-UW"
      },
      "source": [
        " A channel is a feature container that contains same type of infromation. it is also called as feature map. For example: Let's consider a colored/RGB image. It consists of three channels i.e, red, green and blue channel. These three channels are superimposed to form a colored image. A red channels consists of information only about red or shades of red. An image can be represented using any no of channels. A grey image consists of only one channel.\n",
        "   \n",
        "   A kernel is a feature extractor that extracts the relevant features from the images passed as input. All the extracted features which are similar and relevant are kept in layers. These layers consisting of similar and relevant features extracted by kernels are called as channels. Here a kernel is a matrix that slides on top of an image and multiplies corresponding pixels values and finally sums up all values to get a new pixel value. A kernel is initialized randomly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-wu_b8UdGhF"
      },
      "source": [
        "### **2. Why should we (nearly) always use 3x3 kernels?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTs4OZL5dbJ2"
      },
      "source": [
        "In CNN, a kernel size is a hyper parameter to choose from and we choose between smaller filter size to larger filter size. Generally a smaller filter size is chosen as it has a smaller receptive field and extracts the features that are highly local hence this helps in capturing complex features of an image. In smaller filter size, we always use 3x3 kernels because\n",
        "   1. It has lower no of weights and more no of layers.\n",
        "   2. Due to lower no of weights, this is computationally efficient.\n",
        "   3. With  more no of layers it learns complex, more non-linear features\n",
        "   4. 3x3 is an odd number filter and odd filters are preferable because if we were to consider the final output pixel (of next layer) that was obtained by convolving on the previous layer pixels, all the previous layer pixels would be symmetrically around the output pixel. Without this symmetry, we will have to account for distortions across the layers. This will happen due to the usage of an even sized kernel. Therefore, even sized kernel filters arenâ€™t preferred\n",
        "   \n",
        " beside these benfits, we will achieve the same impact as we would achieve with the large filter size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrv6zFqGd87k"
      },
      "source": [
        "**3. How many times do we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ2MHK3UeMFm"
      },
      "source": [
        "199x199 > 197x197 > 195X195 > 193x193 > 191x191 > 189x189 > 187x187 > 185x185 > 183x183 > 181x181 > 179x179 > 177x177 > 175x175 > 173x173 > 171x171 > 169x169 > 167x167 > 165x165 > 163x163 > 161x161 > 159x159 > 157x157 > 155x155 > 153x153 > 151x151 > 149x149 > 147x147 > 145x145 > 143x143 > 141x141 > 139x139 > 137x137 > 135x135 > 133x133 > 131x131 > 129x129 > 127x127 > 125x125 > 123x123 > 121x121 > 119x119 > 117x117 > 115x115 > 113x113 > 111x111 > 109x109 > 107x107 > 105x105 > 103x103 > 101x101 > 99x99 > 97x97 > 95x95 > 93x93 > 91x91 > 89x89 > 87x87 > 85x85 > 83x83 > 81x81 > 79x79 > 77x77 > 75x75 > 73x73 > 71x71 > 69x69 > 67x67 > 65x65 > 63x63 > 61x61 > 59x59 > 57x57 > 55x55 > 53x53 > 51x51 > 49x49 > 47x47 > 45x45 > 43x43 > 41x41 > 39x39 > 37x37 > 35x35 > 33x33 > 31x31 > 29x29 > 27x27 > 25x25 > 23x23 > 21x21 > 19x19 > 17x17 > 15x15 > 13x13 > 11x11 > 9x9 > 7x7 > 5x5 > 3x3 > 1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7pU2Bd7dtxW"
      },
      "source": [
        "## 4. How are kernels initialized? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyD_9jPJeg-2"
      },
      "source": [
        "Kernels for each layer are initialized in different methods.\n",
        "   1. Zero intialization\n",
        "   2. Random initialization\n",
        "   3. He-et-al initialization and Xavior/Glorot initialization.\n",
        "   \n",
        "   1. Zero initialization:\n",
        "      Zero initialization serves no purpose. The neural net does not perform symmetry-breaking.If we set all the weights to be zero, then all the the neurons of all the layers performs the same calculation, giving the same output and there by making the whole deep net useless.\n",
        "   \n",
        "   2. Random initialization:\n",
        "      This serves the process of symmetry-breaking and gives much better accuracy. In this method, the weights are initialized very close to zero, but randomly. This helps in breaking symmetry and every neuron is no longer performing the same computation. But this is just not enough because with just randomly thrown numbers we might encounter either vanishing gradient or exploding gradient. The solution to this problem is to initialize the weights with a specific range\n",
        "      \n",
        "    3. He-et-al initialization and Xavior/Glorot initialization.\n",
        "       These are two popular weight initialization. The weights are still random but differ in range depending on the size of the previous layer of neurons. This provides a controlled initialisation hence the faster and more efficient gradient descent. These two initialization schemes initialize the variance of each layer so that the output distribution of each neuron is the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-lK8OUOemtF"
      },
      "source": [
        "**5. What happens during the training of a DNN?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJmSDPQferaB"
      },
      "source": [
        "During the traing of a DNN, an image is passed to the input layer which in turn passes this to the hidden layer. A kernel is initialized randomly that slides on top of this image and features are extracted. we can define any no of kernels and each kernel produces one feature map(or channel). Again these feature maps are passed to the next layer where again features are extracted. First block layers extract edges and gradients. Second  block layers extract textures and patterns. Third block layers extract parts of objects and fourth block layers extracts objects. Hence every neural network consists of 4 blocks and each block may consist any no of layers. The final object from fourth block is passed to a fully  connected layer which classifies or segments the input image. This whole process of moving from input layer to the output layer is called Forward Propagation. Now the final output is compared with the actual output and the loss is calculated. This loss function is used to compare and measure how good/bad our prediction result was in relation to the actual result. \n",
        "   Once the loss has been calculated, this information is propagated backwards. This process is called backpropagation.\n",
        "In back propagation, it adjusts the weights of the kernel to get a minimal or zero loss. This optimization function is called gradient descent. Thus this completes one epoch. Each epoch is a combination of one forward propagation and one backpropagation. Training can be done for any no of epochs. "
      ]
    }
  ]
}